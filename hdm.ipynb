{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f560d32",
   "metadata": {},
   "source": [
    "# **Heart Disease**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa78f724",
   "metadata": {},
   "source": [
    "## Tabela de Conteúdo\n",
    "1) Importação de bibliotecas + dataset\n",
    "2) EDA (Análise Exploratória de Dados)\n",
    "3) Pré-processamento de Dados\n",
    "    - Limpeza de Dados\n",
    "    - Normalização de Dados\n",
    "    - Codificação de Dados\n",
    "    - Engenharia de Features\n",
    "    - Divisão de Dados\n",
    "    - Aumento de Dados\n",
    "4) Seleção do Modelo\n",
    "    - Treinamento de Modelos (usando validação cruzada)\n",
    "    - Comparação de Modelos (múltiplos algoritmos)\n",
    "    - Otimização de Hiperparâmetros\n",
    "    - Avaliação do Modelo\n",
    "5) Salvamento do Melhor Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965ecaef",
   "metadata": {},
   "source": [
    "## 1) Importação de bibliotecas e dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b81df5b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <ins><a href=\"https://ydata.ai/register\">Upgrade to ydata-sdk</a></ins>\n",
       "                <p>\n",
       "                    Improve your data and profiling with ydata-sdk, featuring data quality scoring, redundancy detection, outlier identification, text validation, and synthetic data generation.\n",
       "                </p>\n",
       "            </div>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import ydata_profiling as pp\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,roc_curve,classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from mlxtend.classifier import StackingCVClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb5a4dac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>204.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  sex   cp  trestbps   chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "0  63.0  1.0  1.0     145.0  233.0  1.0      2.0    150.0    0.0      2.3   \n",
       "1  67.0  1.0  4.0     160.0  286.0  0.0      2.0    108.0    1.0      1.5   \n",
       "2  67.0  1.0  4.0     120.0  229.0  0.0      2.0    129.0    1.0      2.6   \n",
       "3  37.0  1.0  3.0     130.0  250.0  0.0      0.0    187.0    0.0      3.5   \n",
       "4  41.0  0.0  2.0     130.0  204.0  0.0      2.0    172.0    0.0      1.4   \n",
       "\n",
       "   slope   ca thal  target  \n",
       "0    3.0  0.0  6.0       0  \n",
       "1    2.0  3.0  3.0       2  \n",
       "2    2.0  2.0  7.0       1  \n",
       "3    3.0  0.0  3.0       0  \n",
       "4    1.0  0.0  3.0       0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./db/processed.cleveland.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2abdcac",
   "metadata": {},
   "source": [
    "## 2) Análise Exploratória de Dados (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2724529c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dataset shape: {data.shape}\")\n",
    "print(\"\\nData types:\")\n",
    "print(data.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6aaadaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nMissing values:\")\n",
    "print(data.isnull().sum())\n",
    "\n",
    "print(\"\\nBasic statistics:\")\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c046a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTarget variable distribution:\")\n",
    "print(data['target'].value_counts())\n",
    "print(data['target'].value_counts(normalize=True).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b09524a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 10))\n",
    "correlation_matrix = data.apply(lambda x: pd.factorize(x)[0] if x.dtype == 'object' else x).corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d9d30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = data.select_dtypes(include=['int64', 'float64']).columns\n",
    "plt.figure(figsize=(15, 12))\n",
    "for i, feature in enumerate(numerical_features):\n",
    "    plt.subplot(4, 3, i+1)\n",
    "    sns.histplot(data=data, x=feature, hue='target', kde=True)\n",
    "    plt.title(f'Distribution of {feature}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06af55c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = data.select_dtypes(include=['object']).columns.drop('target') if 'target' in data.select_dtypes(include=['object']).columns else data.select_dtypes(include=['object']).columns\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, feature in enumerate(categorical_features):\n",
    "    plt.subplot(4, 2, i+1)\n",
    "    sns.countplot(data=data, x=feature, hue='target')\n",
    "    plt.title(f'Distribution of {feature}')\n",
    "    plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9703d3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(data=data, x='age', hue='target', fill=True)\n",
    "plt.title('Age Distribution by Heart Disease Status')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb79707",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "subplot_count = 0\n",
    "for feature in numerical_features:\n",
    "    if feature != 'sex' and feature != 'fbs' and feature != 'exang':\n",
    "        subplot_count += 1\n",
    "        plt.subplot(3, 3, subplot_count)\n",
    "        sns.boxplot(data=data, x='target', y=feature)\n",
    "        plt.title(f'{feature} by Heart Disease Status')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a53314",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = pp.ProfileReport(data, title=\"Heart Disease Dataset Profiling Report\")\n",
    "profile.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c01e0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.replace('?', \"NA\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8fb806",
   "metadata": {},
   "source": [
    "## 3) Pré-processamento de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbccfc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = data.isnull().sum()\n",
    "print(\"Missing values in each column:\")\n",
    "print(missing_values[missing_values > 0])\n",
    "\n",
    "data['ca'] = pd.to_numeric(data['ca'], errors='coerce')\n",
    "data['ca'].fillna(data['ca'].median(), inplace=True)\n",
    "\n",
    "data['thal'].fillna(data['thal'].mode()[0], inplace=True)\n",
    "\n",
    "print(\"\\nMissing values after imputation:\")\n",
    "print(data.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ff446e",
   "metadata": {},
   "source": [
    "### Limpeza de Dados\n",
    "\n",
    "Converta variáveis ​​categóricas em formato numérico para modelos de aprendizado de máquina."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3f9e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = data.select_dtypes(include=['object']).columns\n",
    "print(\"Categorical columns:\", categorical_cols.tolist())\n",
    "\n",
    "data_encoded = pd.get_dummies(data, columns=['cp', 'thal'], drop_first=True)\n",
    "\n",
    "data_encoded['target'] = (data_encoded['target'] > 0).astype(int)\n",
    "\n",
    "print(\"\\nShape after encoding:\", data_encoded.shape)\n",
    "print(\"\\nEncoded columns:\", data_encoded.columns.tolist())\n",
    "\n",
    "data_encoded = data_encoded.dropna(subset=['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b47e170",
   "metadata": {},
   "source": [
    "### Normalização de Dados\n",
    "\n",
    "Padronize características numéricas para que média = 0 e desvio padrão = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be40d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features_to_scale = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'ca']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "data_encoded[numerical_features_to_scale] = scaler.fit_transform(data_encoded[numerical_features_to_scale])\n",
    "\n",
    "data_encoded[numerical_features_to_scale].describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b69b93",
   "metadata": {},
   "source": [
    "### Engenharia de Features\n",
    "\n",
    "Cria novos recursos que possam ser úteis para previsão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde5cca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encoded['AgeGroup'] = pd.cut(data['age'], bins=[0, 40, 55, 65, 100], \n",
    "labels=['Young', 'Middle-aged', 'Senior', 'Elderly'])\n",
    "\n",
    "data_encoded = pd.get_dummies(data_encoded, columns=['AgeGroup'], drop_first=True)\n",
    "\n",
    "data_encoded['BP_per_Age'] = data['trestbps'] / data['age']\n",
    "data_encoded['HR_per_Age'] = data['thalach'] / data['age']\n",
    "\n",
    "print(\"Dataset shape after feature engineering:\", data_encoded.shape)\n",
    "data_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cbc249",
   "metadata": {},
   "source": [
    "### Divisão de Dados\n",
    "\n",
    "Divide o conjunto de dados em conjuntos de treinamento e teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3830244",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_encoded.drop('target', axis=1)\n",
    "y = data_encoded['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "print(\"\\nClass distribution in training set:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(\"\\nClass distribution in test set:\")\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9cff26",
   "metadata": {},
   "source": [
    "### Aumento de Dados\n",
    "\n",
    "Implementa técnicas de aumento de dados para lidar com desequilíbrio de classes, se necessário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315a8c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = y_train.value_counts()\n",
    "print(\"Class distribution before augmentation:\")\n",
    "print(class_counts)\n",
    "\n",
    "if abs(class_counts[0] - class_counts[1]) / len(y_train) > 0.2:  # If imbalance exceeds 20%\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    \n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    print(\"\\nClass distribution after SMOTE:\")\n",
    "    print(pd.Series(y_train_resampled).value_counts())\n",
    "    \n",
    "    X_train = X_train_resampled\n",
    "    y_train = y_train_resampled\n",
    "    \n",
    "    print(f\"\\nNew X_train shape: {X_train.shape}\")\n",
    "else:\n",
    "    print(\"\\nNo significant class imbalance detected. Skipping augmentation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90bfa36",
   "metadata": {},
   "source": [
    "## 4) Seleção do Modelo\n",
    "\n",
    "Nesta seção, construiremos e avaliaremos múltiplos modelos de aprendizado de máquina para predição de doenças cardíacas:\n",
    "1. Treinar modelos usando validação cruzada\n",
    "2. Comparar o desempenho dos modelos\n",
    "3. Ajustar os hiperparâmetros dos melhores modelos\n",
    "4. Avaliar o desempenho final do modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309e2913",
   "metadata": {},
   "source": [
    "### Treinamento de Modelos (usando validação cruzada)\n",
    "\n",
    "Treinaremos vários modelos de classificação usando validação cruzada k-fold para obter uma estimativa confiável de seu desempenho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea7473b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Support Vector Machine': SVC(probability=True, random_state=42),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'XGBoost': XGBClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "cv_results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    cv_accuracy = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "    cv_roc_auc = cross_val_score(model, X_train, y_train, cv=cv, scoring='roc_auc')\n",
    "    cv_f1 = cross_val_score(model, X_train, y_train, cv=cv, scoring='f1')\n",
    "\n",
    "    cv_results[name] = {\n",
    "        'Accuracy': cv_accuracy,\n",
    "        'ROC-AUC': cv_roc_auc,\n",
    "        'F1 Score': cv_f1\n",
    "    }\n",
    "    \n",
    "    print(f\"Model: {name}\")\n",
    "    print(f\"Mean Accuracy: {cv_accuracy.mean():.4f} (±{cv_accuracy.std():.4f})\")\n",
    "    print(f\"Mean ROC-AUC: {cv_roc_auc.mean():.4f} (±{cv_roc_auc.std():.4f})\")\n",
    "    print(f\"Mean F1 Score: {cv_f1.mean():.4f} (±{cv_f1.std():.4f})\")\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f7bf6c",
   "metadata": {},
   "source": [
    "### Comparação de Modelos\n",
    "\n",
    "Vamos comparar o desempenho de diferentes modelos com base nos resultados da validação cruzada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778c8206",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_accuracy_df = pd.DataFrame({name: results['Accuracy'] for name, results in cv_results.items()})\n",
    "cv_roc_auc_df = pd.DataFrame({name: results['ROC-AUC'] for name, results in cv_results.items()})\n",
    "cv_f1_df = pd.DataFrame({name: results['F1 Score'] for name, results in cv_results.items()})\n",
    "\n",
    "cv_means = pd.DataFrame({\n",
    "    'Mean Accuracy': cv_accuracy_df.mean(),\n",
    "    'Mean ROC-AUC': cv_roc_auc_df.mean(),\n",
    "    'Mean F1 Score': cv_f1_df.mean()\n",
    "}).sort_values(by='Mean ROC-AUC', ascending=False)\n",
    "\n",
    "cv_std = pd.DataFrame({\n",
    "    'Std Accuracy': cv_accuracy_df.std(),\n",
    "    'Std ROC-AUC': cv_roc_auc_df.std(),\n",
    "    'Std F1 Score': cv_f1_df.std()\n",
    "})\n",
    "\n",
    "print(\"Models ranked by ROC-AUC score:\")\n",
    "print(cv_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd6439a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 6))\n",
    "ax = sns.boxplot(data=cv_accuracy_df)\n",
    "ax.set_title('Cross-Validation Accuracy Comparison', fontsize=16)\n",
    "ax.set_xlabel('Model', fontsize=12)\n",
    "ax.set_ylabel('Accuracy', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "ax = sns.boxplot(data=cv_roc_auc_df)\n",
    "ax.set_title('Cross-Validation ROC-AUC Comparison', fontsize=16)\n",
    "ax.set_xlabel('Model', fontsize=12)\n",
    "ax.set_ylabel('ROC-AUC', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "ax = sns.boxplot(data=cv_f1_df)\n",
    "ax.set_title('Cross-Validation F1 Score Comparison', fontsize=16)\n",
    "ax.set_xlabel('Model', fontsize=12)\n",
    "ax.set_ylabel('F1 Score', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d1f832",
   "metadata": {},
   "source": [
    "### Otimização de Hiperparâmetros\n",
    "\n",
    "Vamos selecionar os modelos de melhor desempenho da nossa comparação e otimizar seus hiperparâmetros usando a Pesquisa em Grade ou a Pesquisa Aleatória com um espaço de parâmetros simplificado para evitar problemas de memória."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd25a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer, roc_auc_score\n",
    "\n",
    "print(\"Tuning XGBoost hyperparameters...\")\n",
    "xgb_param_grid = {\n",
    "    'learning_rate': [0.1],\n",
    "    'max_depth': [3, 5],\n",
    "    'n_estimators': [100]\n",
    "}\n",
    "\n",
    "try:\n",
    "    xgb_random = RandomizedSearchCV(\n",
    "        estimator=XGBClassifier(random_state=42),\n",
    "        param_distributions=xgb_param_grid,\n",
    "        n_iter=2,\n",
    "        scoring='roc_auc',\n",
    "        cv=3,\n",
    "        verbose=1,\n",
    "        random_state=42,\n",
    "        n_jobs=1\n",
    "    )\n",
    "\n",
    "    xgb_random.fit(X_train, y_train)\n",
    "\n",
    "    print(f\"Best parameters: {xgb_random.best_params_}\")\n",
    "    print(f\"Best score: {xgb_random.best_score_:.4f}\")\n",
    "    print(\"-\"*50)\n",
    "\n",
    "    best_xgb = xgb_random.best_estimator_\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during XGBoost tuning: {e}\")\n",
    "    print(\"Using default XGBoost model instead\")\n",
    "    best_xgb = XGBClassifier(\n",
    "        random_state=42,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=3,\n",
    "        n_estimators=100\n",
    "    )\n",
    "    best_xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351caab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tuning Random Forest hyperparameters...\")\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [100],\n",
    "    'max_depth': [None, 10],\n",
    "    'min_samples_split': [2]\n",
    "}\n",
    "\n",
    "try:\n",
    "    rf_random = RandomizedSearchCV(\n",
    "        estimator=RandomForestClassifier(random_state=42),\n",
    "        param_distributions=rf_param_grid,\n",
    "        n_iter=2,\n",
    "        scoring='roc_auc',\n",
    "        cv=3,\n",
    "        verbose=1,\n",
    "        random_state=42,\n",
    "        n_jobs=1\n",
    "    )\n",
    "\n",
    "    rf_random.fit(X_train, y_train)\n",
    "\n",
    "    print(f\"Best parameters: {rf_random.best_params_}\")\n",
    "    print(f\"Best score: {rf_random.best_score_:.4f}\")\n",
    "    print(\"-\"*50)\n",
    "\n",
    "    best_rf = rf_random.best_estimator_\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during Random Forest tuning: {e}\")\n",
    "    print(\"Using default Random Forest model instead\")\n",
    "    best_rf = RandomForestClassifier(\n",
    "        random_state=42,\n",
    "        n_estimators=100\n",
    "    )\n",
    "    best_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb72658",
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "print(\"Tuning Logistic Regression hyperparameters...\")\n",
    "\n",
    "lr_param_grid = [\n",
    "    {'penalty': ['l2'], 'C': [0.1, 1, 10], 'solver': ['lbfgs'], 'max_iter': [1000]},\n",
    "    {'penalty': ['none'], 'solver': ['lbfgs'], 'max_iter': [1000]}\n",
    "]\n",
    "\n",
    "try:\n",
    "    lr_random = RandomizedSearchCV(\n",
    "        estimator=LogisticRegression(random_state=42),\n",
    "        param_distributions=lr_param_grid,\n",
    "        n_iter=2,\n",
    "        scoring='roc_auc',\n",
    "        cv=3,\n",
    "        verbose=1,\n",
    "        random_state=42,\n",
    "        n_jobs=1\n",
    "    )\n",
    "\n",
    "    lr_random.fit(X_train, y_train)\n",
    "\n",
    "    print(f\"Best parameters: {lr_random.best_params_}\")\n",
    "    print(f\"Best score: {lr_random.best_score_:.4f}\")\n",
    "    print(\"-\"*50)\n",
    "\n",
    "    best_lr = lr_random.best_estimator_\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during Logistic Regression tuning: {e}\")\n",
    "    print(\"Using default Logistic Regression model instead\")\n",
    "    best_lr = LogisticRegression(\n",
    "        random_state=42,\n",
    "        max_iter=1000\n",
    "    )\n",
    "    best_lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386e6c3c",
   "metadata": {},
   "source": [
    "### Avaliação do Modelo\n",
    "\n",
    "Agora, vamos avaliar nossos modelos ajustados no conjunto de testes para avaliar seu desempenho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75794112",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'roc_auc': roc_auc,\n",
    "        'confusion_matrix': confusion_matrix(y_test, y_pred),\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba\n",
    "    }\n",
    "\n",
    "print(\"Evaluating the best models on the test set...\")\n",
    "\n",
    "xgb_eval = evaluate_model(best_xgb, X_test, y_test)\n",
    "print(\"XGBoost Performance:\")\n",
    "print(f\"Accuracy: {xgb_eval['accuracy']:.4f}\")\n",
    "print(f\"Precision: {xgb_eval['precision']:.4f}\")\n",
    "print(f\"Recall: {xgb_eval['recall']:.4f}\")\n",
    "print(f\"F1 Score: {xgb_eval['f1_score']:.4f}\")\n",
    "print(f\"ROC-AUC: {xgb_eval['roc_auc']:.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(xgb_eval['confusion_matrix'])\n",
    "print(\"-\"*50)\n",
    "\n",
    "rf_eval = evaluate_model(best_rf, X_test, y_test)\n",
    "print(\"Random Forest Performance:\")\n",
    "print(f\"Accuracy: {rf_eval['accuracy']:.4f}\")\n",
    "print(f\"Precision: {rf_eval['precision']:.4f}\")\n",
    "print(f\"Recall: {rf_eval['recall']:.4f}\")\n",
    "print(f\"F1 Score: {rf_eval['f1_score']:.4f}\")\n",
    "print(f\"ROC-AUC: {rf_eval['roc_auc']:.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(rf_eval['confusion_matrix'])\n",
    "print(\"-\"*50)\n",
    "\n",
    "lr_eval = evaluate_model(best_lr, X_test, y_test)\n",
    "print(\"Logistic Regression Performance:\")\n",
    "print(f\"Accuracy: {lr_eval['accuracy']:.4f}\")\n",
    "print(f\"Precision: {lr_eval['precision']:.4f}\")\n",
    "print(f\"Recall: {lr_eval['recall']:.4f}\")\n",
    "print(f\"F1 Score: {lr_eval['f1_score']:.4f}\")\n",
    "print(f\"ROC-AUC: {lr_eval['roc_auc']:.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(lr_eval['confusion_matrix'])\n",
    "print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319053e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "models_eval = [\n",
    "    ('XGBoost', xgb_eval, 'blue'),\n",
    "    ('Random Forest', rf_eval, 'green'),\n",
    "    ('Logistic Regression', lr_eval, 'red')\n",
    "]\n",
    "\n",
    "for name, eval_results, color in models_eval:\n",
    "    fpr, tpr, _ = roc_curve(y_test, eval_results['y_pred_proba'])\n",
    "    roc_auc = eval_results['roc_auc']\n",
    "    \n",
    "    plt.plot(fpr, tpr, color=color, lw=2, label=f'{name} (AUC = {roc_auc:.3f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3827bd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "models_eval = [\n",
    "    ('XGBoost', xgb_eval, axes[0]),\n",
    "    ('Random Forest', rf_eval, axes[1]),\n",
    "    ('Logistic Regression', lr_eval, axes[2])\n",
    "]\n",
    "\n",
    "for name, eval_results, ax in models_eval:\n",
    "    cm = eval_results['confusion_matrix']\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
    "    \n",
    "    ax.set_xlabel('Predicted label')\n",
    "    ax.set_ylabel('True label')\n",
    "    ax.set_title(f'{name} Confusion Matrix')\n",
    "    ax.set_xticklabels(['Negative', 'Positive'])\n",
    "    ax.set_yticklabels(['Negative', 'Positive'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8040b3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_comparison = pd.DataFrame({\n",
    "    'XGBoost': [\n",
    "        xgb_eval['accuracy'],\n",
    "        xgb_eval['precision'],\n",
    "        xgb_eval['recall'],\n",
    "        xgb_eval['f1_score'],\n",
    "        xgb_eval['roc_auc']\n",
    "    ],\n",
    "    'Random Forest': [\n",
    "        rf_eval['accuracy'],\n",
    "        rf_eval['precision'],\n",
    "        rf_eval['recall'],\n",
    "        rf_eval['f1_score'],\n",
    "        rf_eval['roc_auc']\n",
    "    ],\n",
    "    'Logistic Regression': [\n",
    "        lr_eval['accuracy'],\n",
    "        lr_eval['precision'],\n",
    "        lr_eval['recall'],\n",
    "        lr_eval['f1_score'],\n",
    "        lr_eval['roc_auc']\n",
    "    ]\n",
    "}, index=['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC-AUC'])\n",
    "\n",
    "print(\"Model Comparison Summary:\")\n",
    "print(model_comparison)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "model_comparison.plot(kind='bar', figsize=(12, 8))\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.ylabel('Score')\n",
    "plt.xticks(rotation=0)\n",
    "plt.ylim(0, 1.0)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.legend(title='Models')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325c01a0",
   "metadata": {},
   "source": [
    "### Análise de Importância de Recursos\n",
    "\n",
    "Vamos analisar quais características contribuem mais para as previsões do nosso melhor modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919eaeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = best_xgb\n",
    "\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': best_model.feature_importances_\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='Importance', y='Feature', data=feature_importance)\n",
    "    plt.title('Feature Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "elif hasattr(best_model, 'coef_'):\n",
    "    coefficients = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Coefficient': best_model.coef_[0]\n",
    "    }).sort_values(by='Coefficient', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='Coefficient', y='Feature', data=coefficients)\n",
    "    plt.title('Feature Coefficients')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Feature importance not available for this model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c69c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Predição de Doenças Cardiovasculares com MLP\\n\",\n",
    "    \"\\n\",\n",
    "    \"Este notebook implementa o proposto no TCC: treina uma rede neural MLP e compara seu desempenho com a Regressão Logística usando a base de dados pública UCI Heart Disease.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 1. Importação de Bibliotecas e Dados\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"from sklearn.model_selection import train_test_split\\n\",\n",
    "    \"from sklearn.preprocessing import StandardScaler\\n\",\n",
    "    \"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve\\n\",\n",
    "    \"from sklearn.linear_model import LogisticRegression\\n\",\n",
    "    \"from sklearn.neural_network import MLPClassifier\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import seaborn as sns\"\n",
    "   ],\n",
    "   \"execution_count\": null,\n",
    "   \"outputs\": []\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Carregar o dataset\\n\",\n",
    "    \"data = pd.read_csv('./processed.cleveland.csv')\\n\",\n",
    "    \"data['ca'] = pd.to_numeric(data['ca'], errors='coerce')\\n\",\n",
    "    \"data['ca'].fillna(data['ca'].median(), inplace=True)\\n\",\n",
    "    \"data['thal'].fillna(data['thal'].mode()[0], inplace=True)\\n\",\n",
    "    \"# Binarizar target: 0 = sem doença, 1 = com doença\\n\",\n",
    "    \"data['target'] = (data['target'] > 0).astype(int)\\n\",\n",
    "    \"data = data.dropna(subset=['target'])\"\n",
    "   ],\n",
    "   \"execution_count\": null,\n",
    "   \"outputs\": []\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 2. Pré-processamento\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# One-hot encoding para variáveis categóricas\\n\",\n",
    "    \"data_encoded = pd.get_dummies(data, columns=['cp', 'thal'], drop_first=True)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Normalização dos dados numéricos\\n\",\n",
    "    \"numerical_features = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'ca']\\n\",\n",
    "    \"scaler = StandardScaler()\\n\",\n",
    "    \"data_encoded[numerical_features] = scaler.fit_transform(data_encoded[numerical_features])\"\n",
    "   ],\n",
    "   \"execution_count\": null,\n",
    "   \"outputs\": []\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Separar features e target\\n\",\n",
    "    \"X = data_encoded.drop('target', axis=1)\\n\",\n",
    "    \"y = data_encoded['target']\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Dividir em treino e teste\\n\",\n",
    "    \"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\"\n",
    "   ],\n",
    "   \"execution_count\": null,\n",
    "   \"outputs\": []\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 3. Treinamento dos Modelos\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Regressão Logística\\n\",\n",
    "    \"lr = LogisticRegression(max_iter=1000, random_state=42)\\n\",\n",
    "    \"lr.fit(X_train, y_train)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# MLP\\n\",\n",
    "    \"mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=42)\\n\",\n",
    "    \"mlp.fit(X_train, y_train)\"\n",
    "   ],\n",
    "   \"execution_count\": null,\n",
    "   \"outputs\": []\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 4. Avaliação dos Modelos\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"def evaluate_model(model, X_test, y_test):\\n\",\n",
    "    \"    y_pred = model.predict(X_test)\\n\",\n",
    "    \"    y_pred_proba = model.predict_proba(X_test)[:, 1]\\n\",\n",
    "    \"    return {\\n\",\n",
    "    \"        'accuracy': accuracy_score(y_test, y_pred),\\n\",\n",
    "    \"        'precision': precision_score(y_test, y_pred),\\n\",\n",
    "    \"        'recall': recall_score(y_test, y_pred),\\n\",\n",
    "    \"        'f1': f1_score(y_test, y_pred),\\n\",\n",
    "    \"        'roc_auc': roc_auc_score(y_test, y_pred_proba),\\n\",\n",
    "    \"        'confusion_matrix': confusion_matrix(y_test, y_pred),\\n\",\n",
    "    \"        'y_pred_proba': y_pred_proba\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"\\n\",\n",
    "    \"lr_eval = evaluate_model(lr, X_test, y_test)\\n\",\n",
    "    \"mlp_eval = evaluate_model(mlp, X_test, y_test)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print('Logistic Regression:', lr_eval)\\n\",\n",
    "    \"print('MLP:', mlp_eval)\"\n",
    "   ],\n",
    "   \"execution_count\": null,\n",
    "   \"outputs\": []\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 5. Comparação Visual\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# ROC Curve\\n\",\n",
    "    \"plt.figure(figsize=(8,6))\\n\",\n",
    "    \"for name, eval_result, color in [\\n\",\n",
    "    \"    ('Logistic Regression', lr_eval, 'blue'),\\n\",\n",
    "    \"    ('MLP', mlp_eval, 'red')]:\\n\",\n",
    "    \"    fpr, tpr, _ = roc_curve(y_test, eval_result['y_pred_proba'])\\n\",\n",
    "    \"    plt.plot(fpr, tpr, label=f'{name} (AUC={eval_result[\\\"roc_auc\\\"]:.2f})', color=color)\\n\",\n",
    "    \"plt.plot([0,1],[0,1],'k--')\\n\",\n",
    "    \"plt.xlabel('False Positive Rate')\\n\",\n",
    "    \"plt.ylabel('True Positive Rate')\\n\",\n",
    "    \"plt.title('ROC Curve')\\n\",\n",
    "    \"plt.legend()\\n\",\n",
    "    \"plt.grid(True)\\n\",\n",
    "    \"plt.show()\"\n",
    "   ],\n",
    "   \"execution_count\": null,\n",
    "   \"outputs\": []\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Matriz de confusão\\n\",\n",
    "    \"fig, axes = plt.subplots(1, 2, figsize=(12, 5))\\n\",\n",
    "    \"for ax, (name, eval_result) in zip(axes, [('Logistic Regression', lr_eval), ('MLP', mlp_eval)]):\\n\",\n",
    "    \"    sns.heatmap(eval_result['confusion_matrix'], annot=True, fmt='d', cmap='Blues', ax=ax)\\n\",\n",
    "    \"    ax.set_title(f'{name} Confusion Matrix')\\n\",\n",
    "    \"    ax.set_xlabel('Predicted')\\n\",\n",
    "    \"    ax.set_ylabel('True')\\n\",\n",
    "    \"plt.tight_layout()\\n\",\n",
    "    \"plt.show()\"\n",
    "   ],\n",
    "   \"execution_count\": null,\n",
    "   \"outputs\": []\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 6. Conclusão\\n\",\n",
    "    \"\\n\",\n",
    "    \"- O notebook treina e compara uma MLP e uma Regressão Logística para predição de doença cardíaca.\\n\",\n",
    "    \"- Métricas e gráficos permitem avaliar qual modelo é mais adequado para o problema.\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"name\": \"python\",\n",
    "   \"version\": \"\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 2\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec38200b",
   "metadata": {},
   "source": [
    "## 5) Salvamento do Melhor Modelo\n",
    "\n",
    "Vamos salvar nosso modelo de melhor desempenho em disco para que ele possa ser usado em produção ou compartilhado com outros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb62af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "model_performances = {\n",
    "    'Logistic Regression': lr_eval['roc_auc'],\n",
    "    'Random Forest': rf_eval['roc_auc'],\n",
    "    'XGBoost': xgb_eval['roc_auc']\n",
    "}\n",
    "\n",
    "best_model_name = max(model_performances, key=model_performances.get)\n",
    "print(f\"The best performing model is: {best_model_name} with ROC-AUC: {model_performances[best_model_name]:.4f}\")\n",
    "\n",
    "model_objects = {\n",
    "    'Logistic Regression': best_lr,\n",
    "    'Random Forest': best_rf,\n",
    "    'XGBoost': best_xgb\n",
    "}\n",
    "\n",
    "best_model = model_objects[best_model_name]\n",
    "\n",
    "save_dir = 'saved_models'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "model_filename = os.path.join(save_dir, f'{best_model_name.replace(\" \", \"_\").lower()}_model.joblib')\n",
    "joblib.dump(best_model, model_filename)\n",
    "\n",
    "scaler_filename = os.path.join(save_dir, 'scaler.joblib')\n",
    "joblib.dump(scaler, scaler_filename)\n",
    "\n",
    "preprocessing_info = {\n",
    "    'numerical_features_to_scale': numerical_features_to_scale,\n",
    "    'feature_names': list(X.columns)\n",
    "}\n",
    "preprocessing_filename = os.path.join(save_dir, 'preprocessing_info.joblib')\n",
    "joblib.dump(preprocessing_info, preprocessing_filename)\n",
    "\n",
    "print(f\"Model saved to {model_filename}\")\n",
    "print(f\"Scaler saved to {scaler_filename}\")\n",
    "print(f\"Preprocessing info saved to {preprocessing_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783a9cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = joblib.load(model_filename)\n",
    "loaded_scaler = joblib.load(scaler_filename)\n",
    "loaded_preprocessing_info = joblib.load(preprocessing_filename)\n",
    "\n",
    "sample_index = 0\n",
    "sample_X = X_test.iloc[[sample_index]]\n",
    "sample_y = y_test.iloc[sample_index]\n",
    "\n",
    "sample_pred = loaded_model.predict(sample_X)[0]\n",
    "sample_pred_proba = loaded_model.predict_proba(sample_X)[0, 1]\n",
    "\n",
    "print(f\"Sample features: {sample_X.values[0]}\")\n",
    "print(f\"True label: {'Heart Disease' if sample_y == 1 else 'No Heart Disease'}\")\n",
    "print(f\"Predicted label: {'Heart Disease' if sample_pred == 1 else 'No Heart Disease'}\")\n",
    "print(f\"Predicted probability of Heart Disease: {sample_pred_proba:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
